# About 

## History

MapReduce has been useful, but the amount of time it takes for the jobs to run can at times be exhaustive. Also, MapReduce jobs only work for a specific set of use cases. There is a need for computing framework that works for a wider set of use cases.
Apache Spark was designed to be a fast, general-purpose, easy-to-use computing platform. It extends the MapReduce model and takes it to a whole other level. The speed comes from the in-memory computations. Applications running in memory allow for much faster processing and response.

## Where spark fits in

Apache Spark is a **fast, in-memory data processing engine** with elegant and expressive development APIs in Scala,Java, and Python and R that allow data workers to efficiently execute machine learning algorithms that 
require fast iterative access to datasets. Spark on Apache Hadoop YARN enables deep integration with Hadoop and other YARN enabled workloads in the enterprise.

# Highlights

* Batch applications
* Robust API (Scala,Java, and Python and R)
* Deep integration with Hadoop and YARN workloads
* Interactive queries
* Process streaming data 
* Can extend upwards into Machine Learning algorithms, SQL, straeming and graph processing
* 
